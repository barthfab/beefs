_target_: src.models.gpt3_module.Gpt3

model: "text-davinci-002"

max_tokens: 512


temperature: 0
#What sampling temperature to use. Higher values means the model will take more risks.
#Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a
#well-defined answer.
top_p: 1
#An alternative to sampling with temperature, called nucleus sampling, where the model
#considers the results of the tokens with top_p probability mass. So 0.1 means only the
#tokens comprising the top 10% probability mass are considered.
n: 1
#How many completions to generate for each prompt.
stream: false
#Whether to stream back partial progress. If set, tokens will be sent as data-only
#server-sent events as they become available, with the stream terminated by a data:
#[DONE] message.
logprobs: null
#Include the log probabilities on the logprobs most likely tokens, as well the chosen
#tokens.
presence_penalty: 0
#Number between -2.0 and 2.0. Positive values penalize new tokens based on whether
#they appear in the text so far, increasing the model's likelihood to talk about new topics.
frequency_penalty: 0
#Number between -2.0 and 2.0. Positive values penalize new tokens based on their
#existing frequency in the text so far, decreasing the model's likelihood to repeat
#the same line verbatim.

stop: "\n"

arg_finder: 0
